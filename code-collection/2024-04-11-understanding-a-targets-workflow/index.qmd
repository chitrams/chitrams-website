---
title: "Understanding a targets workflow: An introduction"
author: "Chitra M Saraswati"
description: "`{targets}`: why use it, how to use it, and some resources to get you started. (Hint: reduce the mental burden of figuring out which step of the analysis you're on.)"
date: "2024-04-11"
date-modified: "2024-04-23"
categories:
  - R
  - workflow
---

I was recently introduced to the R package targets as part of learning how to develop software. As I continue to use it, I've been blown away by how much easier my workflow has become\--even outside of software development\--and I regret not having known of it sooner. It would have been nice to use this workflow when I was doing my Master's thesis, which involved a large amount of complex and repeated analyses. So here's my notes on the targets workflow so far: why use it, how to use it (especially if you've never used it before), and the resources I found most useful.

# Why use a targets workflow?

Most of us doing statistical analysis work on complex projects with moving parts. What I mean by this is: say I have a workflow that involves loading the data, some exploratory data analysis, fitting the data to a model, visualising it, and then creating a summary table of the data. It's nothing too complicated, but it *is* complex: if one part changes, the rest of the project changes too.

For example, perhaps the data is updated and we want to re-run the whole analysis from start to finish. Or perhaps we decide that the model isn't a good fit, so we run some other models and see which one is better; in this instance, only the model and visualisation would change, but the rest of the project stays the same.

All this is fine if you have well-annotated code and know which parts of the script to re-run. But what if the computations are intensive and time-consuming? Or what if it takes a while to render your table because you've included so many variables? Or even if you're just tweaking a few variables and then making a plot; if you miss re-naming a variable earlier on in your script, your plot might not run because it depends on that variable.

It's in situations like these where targets can help you. Using a targets workflow means you'll have it all laid out in front of you instead of needing to look for specific bits of code in your script and any dependencies. It'll take the headache out of figuring out which parts of the analysis depends on which variables and functions. Then when you change a variable name (or any part of the project, really), you'll know which parts of the analysis would need to be re-run, minus the headaches.

To summarise, I find a targets workflow especially useful in the following scenarios:

-   If you're doing something complex that has a lot of moving parts
-   If said moving parts will change as your project progresses
-   If you'd like to re-run your analysis with different inputs (e.g. a new dataset), or to create different outputs (e.g. plots and tables); especially if you're re-running your analysis a lot!

# How to use targets

When you're first starting to program in R--especially if you're mainly doing statistical analysis--a targets workflow might seem an unnecessary way of going about things. When I first came across the targets workflow, I felt overwhelmed and didn't understand how it could be useful for my workflow. So the way I'd recommend going about it is to take an existing project and to convert that to a targets workflow.

The [quick walkthrough](https://books.ropensci.org/targets/walkthrough.html) in the user manual is an excellent example of how to use the targets workflow--I highly recommend reading it and going through the worked example. It does a much better job than I can in giving a reproducible example of how to use targets. Then I recommend converting your project to a targets workflow by using that walkthrough, or this [quick tutorial](https://carpentries-incubator.github.io/targets-workshop/index.html).

I'll walk you through a conceptual overview of how I use targets. I highly recommend you work through the two tutorials above first, though.

First I make sure that I understand my workflow from beginning to end. This overview is the main advantage of using a targets workflow: you'll have a bird's eye view of your analysis and figure out where changes are necessary. For example, I may break down my analysis like this:

-   Read in csv file of data
-   Summarise data
-   Fit a model to the data
-   Create a plot of the model

Once I've done that and would like to start creating a targets workflow, I'll run the following in my console:

```{r, eval=FALSE}
library(targets)
library(tarchetypes)

tar_script()
```

This creates a file in your project directory called `_targets.R`. This is the high-level overview of the project, so I open that and write out something like the following:

```{r, eval=FALSE}
# _targets.R file

# Set-up ------------------------------

library(targets)
library(tarchetypes)

# Load packages needed for this project
tar_option_set(
  packages = c("tidyverse",
               "purrr")
  )

# Set up a workspace when our code errors
tar_option_set(workspace_on_error = TRUE)

# Load functions to be used in our project
source("R/functions.R")

# Target objects ------------------------------

tar_plan(
  
  # Load data
  tar_file(
    path_to_data,
    "./raw/data.csv"
    ),
    
  tar_target(
    data,
    read_csv(path_to_data)
  ),
  
  # Create a summary table
  tar_target(
    summary_table,
    summarise_data(data)
  ),
  
  # Fit a model to the data
  tar_target(
    model,
    model_data(data)
  ),
  
  # Create plot of the data
  tar_target(
    plot,
    plot_data(data)
  )
  
)

```

Where `summary_table`, `model_data`, and `plot_data` are functions I've written up myself and saved in a separate script called `functions.R`.

The advantage of the above is if your data changes, as an example, you just need to change the first target object `path_to_data` to point to the new data file. You can then re-run the whole analysis using `tar_make()` and your resulting summary table, fitted model, and plot will be re-created. This is so much easier than having a script and needing to change `data` to `new_data` for every instance in which it's referred to.

You can also have an overview of your workflow by running `tar_visnetwork()`.

# Final thoughts

Using a targets workflow requires you to change the way you approach programming. If you're trained as a statistical programmer, I assume you're used to writing your code in scripts. A targets workflow requires you to write your code as functions instead (otherwise known as "functional programming").

It *is* a steep learning curve and it did take me a while to get my head around why and how I should use a targets workflow. It's a different way of thinking about how to program.

Despite this, I do highly recommend using a targets workflow instead of having multiple R scripts, for all the reasons I've stated above. Utilising this workflow is a step towards reproducible research, which is always great.

# Resources for learning targets

I think the best way to understand why a targets workflow is useful is to actually do a project within a targets workflow. So here are some resources I found helpful:

-   The targets R package [user manual](https://books.ropensci.org/targets/) with a quick [walkthrough](https://books.ropensci.org/targets/walkthrough.html) to get you started. I highly recommend this one if you're trying to set up a targets workflow by yourself.
-   For a more hands-on tutorial, I recommend this [Carpentries workshop](https://carpentries-incubator.github.io/targets-workshop/index.html) by Joel Nitta. I really liked that this gave you a bare-bones overview of what a targets workflow looks like. I used this the first time I tried to set up a targets workflow, but I also had the benefit of having someone walk me through using targets.

More readings on targets:

-   The official targets [website](https://docs.ropensci.org/targets/)
-   Within the official website: a list of [examples](https://github.com/ropensci/targets#example-projects) for targets workflows
